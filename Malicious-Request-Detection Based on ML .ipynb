{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect malicious and normal request using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using 'time' for time related operation\n",
    "#using 'urlib' for http related operation\n",
    "#using 'html' for html related operation\n",
    "import os\n",
    "import urllib\n",
    "import time \n",
    "import html\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_list(filename):\n",
    "    filepath = \"../input/malicious-request-dataset/\" + filename\n",
    "    data = open(filepath, 'r', encoding='UTF-8').readlines()\n",
    "    query_list = []\n",
    "    for d in data:\n",
    "        # decoding(解码)\n",
    "        d = str(urllib.parse.unquote(d))   # converting url encoded data to simple string\n",
    "        query_list.append(d)\n",
    "    return list(set(query_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining N-grams & TF-IDF to construct feature matrix\n",
    "#### Background knowledge: \n",
    "TF-IDF(Term Frequency-InversDocument Frequency) is useful in information processing & data mining. This technique implements a statistical method to compute the significance of a certain character according to its  frequency of appearing in the text and the frequency of the document appearing in the corpus. It has the advantage of filtering out some common but irrelevant words while retaining the important words that influence the whole text. The larger the TF-IDF value is, the more important the feature is to the whole text. The basic idea is to transform text into feature matrix and reduce the weight of common words (such as we, all, WWW, etc.) so as to better show the value of a text.\n",
    "Consult：https://www.jianshu.com/p/e2a0aea3630c\n",
    " \n",
    "#### N-grams：Consult https://blog.csdn.net/songbinxu/article/details/80209197\n",
    "\n",
    "#### Thought based on our scenario：\n",
    "No matter malicious request & normal request are both lists of variable length strings, making it difficult to process these irregular data directly by logistic regression algorithm. Thus, we need to find the numerical characteristics of these texts to train our detection model. So that is why we talk about TF-IDF. It can be used to show the characteristics of the text and output in the form of digital matrices. Before calculating TD-IDF, the content of each document (URL request) needs to be divided into words, namely defining the length of the entry of these document. Here, n-grams of length 3 is selected, which can be adjusted according to the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Segmentation(分词处理)\n",
    "Since there is no blank space to divide URL request, so first we have to use word segmentation technique to process it. Here we choose N-murgrams with a length of 3, and it can be adjusted according to the accuracy of the model. \n",
    "References：https://www.zhihu.com/question/266054946?sort=created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer function, this will make 3 grams of each query\n",
    "# eg: www.foo.com/1 will be transformed to ['www','ww.','w.f','.fo','foo','oo.','o.c','.co','com','om/','m/1']\n",
    "def get_ngrams(query):\n",
    "    tempQuery = str(query)\n",
    "    ngrams = []\n",
    "    for i in range(0, len(tempQuery)-3):\n",
    "        ngrams.append(tempQuery[i:i+3])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset discryption:\n",
    "Goodqueries.txt: 1265974 pieces of data, derive from the logging request of http://secrepo.com.   \n",
    "Badqueries.txt: 44532 pieces of data, derive from https://github.com/foospidy/payloads, including XSS, SQL injection etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Get normal request and print some expamles\n",
    "    good_query_list = get_query_list('goodqueries.txt')\n",
    "    print(u\"Normal Request: \", len(good_query_list)) # using unicode to encode characters\n",
    "    print(u\"For Example:\")\n",
    "    for  i in range(0, 5):\n",
    "        print(good_query_list[i].strip('\\n'))\n",
    "    print(\"\\n\")\n",
    "        \n",
    "    # Get malicious request and print some examples\n",
    "    bad_query_list = get_query_list('badqueries.txt')\n",
    "    print(u\"Malicious Request: \", len(bad_query_list))\n",
    "    print(u\"For Example:\")\n",
    "    for  i in range(0, 5):\n",
    "        print(bad_query_list[i].strip('\\n'))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Preprocessing (预处理 good_y标记为0 bad_y标记为1)\n",
    "    good_y = [0 for i in range(0, len(good_query_list))]\n",
    "    print(good_y[:5])\n",
    "    bad_y = [1 for i in range(0, len(bad_query_list))]\n",
    "    print(bad_y[:5])\n",
    "    \n",
    "    queries = bad_query_list + good_query_list\n",
    "    y = bad_y + good_y\n",
    "\n",
    "    # converting data to vectors\n",
    "    # TfidfTransformer + CountVectorizer  =  TfidfVectorizer\n",
    "    # sklearn.feature_extraction.text.TfidfVectorizer() is used to convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    vectorizer = TfidfVectorizer(tokenizer=get_ngrams)\n",
    "\n",
    "    # Preparing for model training（把不规律的文本字符串列表转换成规律的 ([i,j], tdidf值) 的矩阵X)\n",
    "    # (用于下一步训练逻辑回归分类器)\n",
    "    X = vectorizer.fit_transform(queries)\n",
    "    print(X.shape)\n",
    "\n",
    "    # Split dataset for teaing and testing\n",
    "    # (使用train_test_split分割X,y列表)\n",
    "    # (X_train矩阵的数目对应y_train列表的数目(一一对应),用来训练模型)\n",
    "    # (X_test矩阵的数目对应y_test列表的数目(一一对应),用来测试模型的准确性)\n",
    "    # Consult: https://blog.csdn.net/qq_39355550/article/details/82688014\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=42)\n",
    "\n",
    "    # Train the model and test it\n",
    "    # (使用逻辑回归方法模型)\n",
    "    LR = LogisticRegression()\n",
    "    # (训练模型)\n",
    "    LR.fit(X_train, y_train)\n",
    "    # (对模型的准确度进行计算)\n",
    "    print('模型的准确度:{}'.format(LR.score(X_test, y_test)))\n",
    "    print(\"\\n\")\n",
    "    # (对新的请求列表进行预测)\n",
    "    new_queries = ['www.foo.com/id=1<script>alert(1)</script>',\n",
    "                   'www.foo.com/name=admin\\' or 1=1','abc.com/admin.php',\n",
    "                   '\"><svg onload=confirm(1)>',\n",
    "                   'test/q=<a href=\"javascript:confirm(1)>',\n",
    "                   'q=../etc/passwd',\n",
    "                   '/stylesheet.php?version=1331749579',\n",
    "                   '/<script>cross_site_scripting.nasl</script>.idc',\n",
    "                   '<img \\x39src=x onerror=\"javascript:alert(1)\">',\n",
    "                   '/jhot.php?rev=2 |less /etc/passwd']\n",
    "    # 矩阵转换\n",
    "    X_predict = vectorizer.transform(new_queries)\n",
    "    res = LR.predict(X_predict)\n",
    "\n",
    "    #Print the result\n",
    "    res_list = []\n",
    "    for q,r in zip(new_queries, res):\n",
    "        tmp = 'Normal Request' if r == 0 else 'Malicious Request'\n",
    "        q_entity = html.escape(q)\n",
    "        # Consult: https://www.jianshu.com/p/d896e3017417\n",
    "        res_list.append({'url':q_entity,'res':tmp})\n",
    "\n",
    "    for n in res_list:\n",
    "        print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
